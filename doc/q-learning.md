# Q-learning

Minecraft上でオセロAIを実行させる方法として、Q-learningによる学習結果を用いることを考えていく。

## Q値

**Q値**($Q(s,a)$)とは「ある状態$s(\in S)$の下で行動$a(\in A)$を取った時の価値」を表す。**状態行動価値**とも。短絡的な報酬ではなく、未来を見据えた長期的な報酬である点に注意。

## Q-learning

強化学習の一つ。
$\Delta t$経った後の状態からの遷移を考える。この遷移の時に割引率$\gamma$(the discount factor)($0\leq\gamma\geq 0$)を掛けるようにする。更に学習率として$\alpha$も定義する。
学習を始める前に$Q$をある固定値に初期化する。そして、その時間下$t$で状態$s_t$から取りうる行動$a_t\in A_t$を一つ選択し、報酬$r_t$(reward)を得て次の状態$s_{t+1}$に移り$Q$を更新する。この$Q$の更新はベルマン方程式から以下のように行われる。

$$
Q_{new}(s_t,a_t) \leftarrow Q_{old}(s_t,a_t)+\alpha(r_t+\gamma \underset{a\in A_{t+1}}{max}Q(s_{t+1},a)-Q_{old}(s_t,a_t))
$$

以下のように書き換えたものを使うことも。

$$
Q_{new}(s_t,a_t) \leftarrow (1-\alpha)Q_{old}(s_t,a_t)+\alpha(r_t+\gamma \underset{a\in A_{t+1}}{max}Q(s_{t+1},a))
$$

また、行動の決定方法としてその状態下$s$で$Q$が最大となる行動$a_t$を選択していくと局所解に陥ってしまうことがあるので、$\epsilon$-greedy法などが行動選択に用いられる。

## オセロでのQ-learning

### 状態s

まず初めに考えることは、盤面をどのように状態$s$に落とし込むかである。ある盤面をそのまま1対1で$s$と対応させてしまうと、少なくとも$2^{64}$以上となってしまいとてもではないが処理できなくなってしまう。そこで盤面の特徴量を抽出していき、$s$の要素数を減らしていくことにする。

#### 候補1

- 各プレイヤーの角のコマの数
- プレイヤーのコマの差
- 空白マスの数

要素数は適当に見積もって、$5\times 5\times 65\times 61 = 99125$

#### 候補2

- 各プレイヤーの確定コマ数
- プレイヤーのコマの差
- 空白マスの数

要素数は適当に見積もって、$64\times 65\times 61 = 253760$

### 報酬r

オセロでは最後に勝敗が決まるので、当然ながらその場その場での勝敗は無い。そこで、ゲームの最後は勝敗もしくは引き分けの三通りによる報酬の決定、それ以外では確定石による報酬を考えてみる。

#### ゲーム終了時

$$
r=\left\{\begin{aligned}
  & 1 & \text{(win)}\\
  & 0 & \text{(draw)}\\
  & -1 & \text{(lose)}
\end{aligned}
  \right.
$$

#### それ以外の時

$d_0$と$d_1$をそれぞれのプレイヤーの確定石とし、プレイヤー0の報酬を考えているとする。

$$
r=\left\{\begin{aligned}
  & 1 & if\quad d_0> 32\\
  & -1 & elif\quad d_1 > 32\\
  & (d_0-d_1)/32 & otherwise
\end{aligned}\right.
$$

但し、確定石の探索にはコストがかかるので盤面の端のみ数え上げることにする。

---

上記の方法で$r$を定めると、コマの数的には負けているにもかかわらず値がプラスになってしまうことがある。これを回避するために、ゲーム終了時以外でもコマの数を$r$に与えることにする。

#### ゲーム終了時以外

$p_0$と$p_1$をそれぞれプレイヤーのコマの数とする。また、盤面の端(28マス)のみで確定石を考える。

$$
r = ((d_0-d_1)/28+(p_0-p_1)/|p_0-p_1|*count/64)/2
$$

## References

強化学習入門 ～これから強化学習を学びたい人のための基礎知識～, <https://blog.brainpad.co.jp/entry/2017/02/24/121500>
Q-learning, <https://en.wikipedia.org/wiki/Q-learning>
